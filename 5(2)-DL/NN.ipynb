{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Instructions:*\n",
    "- **과제 명세서를 읽어주시고 코드 작성을 해주시길 바랍니다**</span> \n",
    "- **명시된 step을 따라가며 전체적인 학습 방법을 숙지합니다**</span>\n",
    "- (**첫 번째 cell 결과로 나온 시간을 기준으로 채점을 하겠습니다**</span>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This code is written at 2026-01-26 21:35:30.535784\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(\"This code is written at \" + str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "\n",
    "Mutilayer Perceptron(```class MutiLayerPerceptron```)으로 간단한 Binary classification task를 진행해볼 것입니다. \n",
    "\n",
    "> 1. **Dataset**\n",
    ">> $\\texttt{moon}$ dataset\n",
    "> 2. **Network architecture**\n",
    "\n",
    " > $H_1 = X \\cdot W_1 + b_1$   \n",
    " > $z_1 = ReLU(H_1)$ where $ReLU$($=\\max(0,x)$) is a rectified linear unit and $z_1$ is an output of the first hidden layer.  \n",
    " > $H_2 = z_1 \\cdot W_2 + b_2$   \n",
    " > $z_2 = LeakyReLU(H_2)$ where $LeakyReLU$($=\\max(0.01x,x)$) and $z_2$ is an output of the second hidden layer. \n",
    " > $H_3 = z_2 \\cdot W_3 + b_3$   \n",
    " > $z_3 = tanh(H_3 + H_1)$ where $\\tanh$ is a tanh function and $z_3$ is an output of the third hidden layer.  \n",
    " > $H_4 = z_3 \\cdot W_4 + b_4$   \n",
    " > $\\hat y = \\sigma(H_4)$ where $\\sigma$ is a sigmoid function unit and $\\hat y$ is an output of the network.\n",
    " \n",
    " > **$W$** and **$b$**는 각각 weights와 bias.    \n",
    " > **weight 초기화**: Standard normal ($\\texttt{np.random.randn}$. 사용)  \n",
    " > **bias 초기화(intercept)**: 0     \n",
    " > **Input size**: 2  \n",
    " > **The first hidden layer size**: 10  \n",
    " > **The second hidden layer size**: 10  \n",
    " > **Output size**: 1   \n",
    " > **Regularization parameter $\\lambda$**: 0.001  \n",
    " > **Loss function**: Binary cross entropy loss (or equivently log loss).  \n",
    " > **Total loss** : \n",
    " > $L_{total} = \\sum_{i=1}^N{ (-y^{(i)}\\log \\hat{y}^{(i)} -(1-y^{(i)})\\log(1-\\hat{y}^{(i)})) } +  \\lambda \\|W\\|^2 $   \n",
    " > **Optimization**: Gradient descent  \n",
    " > **Learning rate** = 0.0001  \n",
    " > **Number of epochs** = 50000  \n",
    " > $y$는 정답, $\\hat{y}$는 예측값이고 0부터 1사이에 존재한다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "\n",
    "from mlp import MultiLayerPerceptron\n",
    "import utils\n",
    "\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Load data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1463b2510>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"STEP 1: Load data\")\n",
    "\n",
    "# Load data\n",
    "X_train, y_train = sklearn.datasets.make_moons(300, noise = 0.25)\n",
    "\n",
    "# Visualize data\n",
    "plt.scatter(X_train[:,0], X_train[:,1], s = 40, c=y_train, cmap=plt.cm.RdYlGn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2: Train the model\n",
      "Loss (epoch 1000): 60.28412762076126\n",
      "Loss (epoch 2000): 43.792121880946745\n",
      "Loss (epoch 3000): 40.0830390664185\n",
      "Loss (epoch 4000): 38.38777406606593\n",
      "Loss (epoch 5000): 37.22537475720977\n",
      "Loss (epoch 6000): 36.37667445268596\n",
      "Loss (epoch 7000): 35.62153535671111\n",
      "Loss (epoch 8000): 34.95771977356007\n",
      "Loss (epoch 9000): 34.38638700769442\n",
      "Loss (epoch 10000): 33.805464212242235\n",
      "Loss (epoch 11000): 33.19370930833573\n",
      "Loss (epoch 12000): 32.67186432838176\n",
      "Loss (epoch 13000): 31.995837895550004\n",
      "Loss (epoch 14000): 31.49694040214882\n",
      "Loss (epoch 15000): 30.966111966694402\n",
      "Loss (epoch 16000): 30.353406016552817\n",
      "Loss (epoch 17000): 29.72764478214013\n",
      "Loss (epoch 18000): 29.1203339507407\n",
      "Loss (epoch 19000): 28.586294123189727\n",
      "Loss (epoch 20000): 28.1067388030415\n",
      "Loss (epoch 21000): 27.71710527471356\n",
      "Loss (epoch 22000): 27.389402890966014\n",
      "Loss (epoch 23000): 27.082806464723358\n",
      "Loss (epoch 24000): 26.83300130448598\n",
      "Loss (epoch 25000): 26.566941278887207\n",
      "Loss (epoch 26000): 26.385349090413346\n",
      "Loss (epoch 27000): 26.16941618822004\n",
      "Loss (epoch 28000): 25.978318156065573\n",
      "Loss (epoch 29000): 25.806124109764564\n",
      "Loss (epoch 30000): 25.35606159044481\n",
      "Loss (epoch 31000): 24.766391293282247\n",
      "Loss (epoch 32000): 24.329286344724427\n",
      "Loss (epoch 33000): 23.983039030628557\n",
      "Loss (epoch 34000): 23.54574332061714\n",
      "Loss (epoch 35000): 23.22922231222107\n",
      "Loss (epoch 36000): 22.996006634313378\n",
      "Loss (epoch 37000): 22.729263163304278\n",
      "Loss (epoch 38000): 22.551857010155018\n",
      "Loss (epoch 39000): 22.328446723002436\n",
      "Loss (epoch 40000): 22.145450603025832\n",
      "Loss (epoch 41000): 21.93439785158283\n",
      "Loss (epoch 42000): 21.76659348822671\n",
      "Loss (epoch 43000): 21.566418044658043\n",
      "Loss (epoch 44000): 21.406740582501218\n",
      "Loss (epoch 45000): 21.227005379049583\n",
      "Loss (epoch 46000): 21.050734276312422\n",
      "Loss (epoch 47000): 20.874519511076556\n",
      "Loss (epoch 48000): 20.711776003245127\n",
      "Loss (epoch 49000): 20.518289642118688\n",
      "Loss (epoch 50000): 20.31342962252236\n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 2: Train the model\")\n",
    "# random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Hyperparameters\n",
    "nn_input_dim = 2\n",
    "nn_output_dim = 1\n",
    "nn_hdim1 = 10\n",
    "nn_hdim2 = 10\n",
    "nn_hdim3 = 10\n",
    "lr = 0.0001 \n",
    "L2_norm = 0.001\n",
    "epoch = 50000\n",
    "\n",
    "model = MultiLayerPerceptron(nn_input_dim, nn_hdim1, nn_hdim2, nn_hdim3, nn_output_dim, init=\"random\")\n",
    "stats = model.train(X_train, y_train, learning_rate=lr, L2_norm=L2_norm, epoch=epoch, print_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 3: Plot decision boundary\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Decision Boundary: Hidden layer dimension (10, 10)')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"STEP 3: Plot decision boundary\")\n",
    "# Plot the decision boundary\n",
    "utils.plot_decision_boundary(lambda x: model.predict(x), X_train, y_train)\n",
    "plt.title(f\"Decision Boundary: Hidden layer dimension {nn_hdim1, nn_hdim2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(np.arange(len(stats['loss_history'])) * 1000, stats['loss_history'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training loss over epoch')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.arange(len(stats['train_acc_history'])) * 1000, stats['train_acc_history'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Clasification accuracy')\n",
    "plt.title('Training accuracy over epoch')\n",
    "plt.gcf().set_size_inches(20, 8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
